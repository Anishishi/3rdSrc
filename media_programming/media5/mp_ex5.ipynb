{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5回課題：tf-idfを使って「銀河鉄道の夜」の重要語を抽出しよう\n",
    "\n",
    "## 入力データの説明\n",
    "\n",
    "`texts/novels_miyazawa_wakati.json`には、宮沢賢治の全小説の分かち書き文がjson形式で記録されています。   \n",
    "記録形式は以下の通りです。\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"author\": \"宮沢賢治\",\n",
    "        \"title\": \"『春と修羅』\",\n",
    "        \"text\": \"目次 \\n『 春 と 修羅 』 \\n春 と...(本文続く）\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "`title`が`銀河鉄道の夜`であるような要素の`text`の値には「銀河鉄道の夜」の本文の分かち書き文が納められています。\n",
    "\n",
    "## 課題１：tf-idfモデルの学習\n",
    "\n",
    "宮沢賢治の全作品を使って、各作品を1文書としたときのtf-idfを学習してください。   \n",
    "tf-idfの学習には、`sklearn.feature_extraction.text`の`TfidfVectorizer`を使ってください。   \n",
    "ここで、TfidfVectorizerのパラメータは以下のようにしてください。\n",
    "-    max_features=1000 (語彙サイズは最大1000個)\n",
    "-    max_df=5　（5つ以下の小説までに現れる語彙）\n",
    "-    min_df=3　（3つ以上の小説に現れる語彙）\n",
    "\n",
    "## 課題２：「銀河鉄道の夜」の重要語の抽出\n",
    "\n",
    "小説「銀河鉄道の夜」のtf-idf値が大きい単語上位10単語とそのtf-idf値を、   \n",
    "各行にカンマ区切りで単語とtf-idf値が並んだ形式で、ex5-tfidf.txtという名前のファイルに出力してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ヒント\n",
    "\n",
    "`TfidfVectorizer`には、各要素が1つの小説の本文全体（単語が半角空文字で区切られた分かち書き文）であるようなリストを渡します。   \n",
    "今回入力とするjsonファイルには249作品が納められているので、`TfidfVectorizer`に渡す行列は249個の要素を持つのベクトルということになります。   \n",
    "このようなリストは以下のプログラムで作成することができます。\n",
    "\n",
    "以下のプログラムでは、\n",
    "\n",
    "1. `texts/novels_miyazawa_wakati.json`の本文（jsonファイルを辞書として読み込んだ時の各要素のキーが`text`）を読み込み\n",
    "'\\n'を空白に置換（replace('\\n', ' '))した後で、`wakati`という名前のリストに追加(append)しています。   \n",
    "つまり、`wakati`の$i$番目の要素は、$i$番目に登録されている小説の本文（全文の分かち書き文）というわけです。   \n",
    "`novels_miyazawa_wakati.json`には249作品が納められているので、このリストは249次元となります。   \n",
    "\n",
    "2. また同時に、`novels_miyazawa_wakati.json`において、ある作品名の小説が何番目に登録されているかを返す辞書`title2ID`を生成しています。   \n",
    "つまり、`title2ID['作品名'] = [登場順]`です。   \n",
    "たとえば「銀河鉄道の夜」は0番目から数えて225番目に現れるので、  \n",
    "    `title2ID['銀河鉄道の夜'] = 225`  \n",
    "となります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小説の数: 249\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "### このセルは変更しないでください！！！ ###\n",
    "############################################\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "file = 'texts/novels_miyazawa_wakati.json'\n",
    "\n",
    "with open(file, 'r', encoding='utf-8') as fi:\n",
    "    novels = json.load(fi)\n",
    "\n",
    "print('小説の数:', len(novels))\n",
    "\n",
    "wakati = []\n",
    "num = 0\n",
    "title2ID = {}\n",
    "for novel in novels:\n",
    "    wakati.append(novel['text'].replace('\\n',' ')) \n",
    "    title2ID[novel['title']] = num\n",
    "    num += 1\n",
    "    \n",
    "print(title2ID['銀河鉄道の夜'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題１：tf-idfモデルの学習\n",
    "\n",
    "以下にtf-idfを学習するプログラムを書いてください。   \n",
    "上のヒントを使ってもいいし使わなくてもいいです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "file = 'texts/novels_miyazawa_wakati.json'\n",
    "\n",
    "with open(file, 'r', encoding='utf-8') as fi:\n",
    "    novels = json.load(fi)\n",
    "wakati = []\n",
    "num = 0\n",
    "title2ID = {}\n",
    "for novel in novels:\n",
    "    wakati.append(novel['text'].replace('\\n',' ')) \n",
    "    title2ID[novel['title']] = num\n",
    "    num += 1\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, max_df=5, min_df=3)\n",
    "corpus=np.array(wakati)##一つ目のブロックを実行してwakatiを求めた後に実行する。\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題２：「銀河鉄道の夜」の重要語の抽出\n",
    "\n",
    "小説「銀河鉄道の夜」のtf-idf値が大きい単語上位10単語とそのtf-idf値を、   \n",
    "**各行にカンマ区切りで単語とtf-idf値が並んだ形式で、`ex5-tfidf.txt`という名前のファイルに出力**してください。\n",
    "\n",
    "ex5-tfidf.txt`は以下のようにります。\n",
    "```\n",
    "神さま,0.447\n",
    "切符,0.289\n",
    "牛乳,0.263\n",
    "ぱいに,0.237\n",
    "十字架,0.22\n",
    "向う岸,0.22\n",
    "白鳥,0.22\n",
    "さそり,0.202\n",
    "烏瓜,0.192\n",
    "もろこし,0.165\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "神さま,0.4102159427505732\n",
      "切符,0.2654338453091944\n",
      "牛乳,0.24130349573563128\n",
      "ぱいに,0.21717314616206815\n",
      "十字架,0.20181235144172602\n",
      "向う岸,0.20181235144172602\n",
      "白鳥,0.20181235144172602\n",
      "さそり,0.18587754855058236\n",
      "烏瓜,0.17658580751151026\n",
      "もろこし,0.15135926358129453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "novel_no=title2ID['銀河鉄道の夜']\n",
    "feature_names = vectorizer.get_feature_names() #単語のリストを獲得します\n",
    "# 単語とそのtf-idf値の対を辞書として登録\n",
    "pair = dict(zip(feature_names, X[novel_no,:].toarray()[0]))\n",
    "# tf-idfの高い順にソートして、単語とtf-idfの対をタプルとしてリスト化する\n",
    "ans_ginga=[(x, pair[x]) for x in sorted(pair, key=lambda x:-pair[x])]\n",
    "\n",
    "output10=''\n",
    "# 上位10件\n",
    "tmp=ans_ginga[:10]\n",
    "for p in tmp:\n",
    "    output10+=p[0]+','+str(p[1])+'\\n'\n",
    "print(output10)\n",
    "with open('ex5-ifidf.txt', 'w') as fw:\n",
    "    fw.write(output10)\n",
    "fw.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
